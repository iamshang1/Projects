from pyspark.sql import SparkSession
from pyspark.mllib.linalg import SparseVector, VectorUDT, Vectors
from pyspark.sql.types import *
import numpy as np

#initialize spark session
spark = SparkSession\
        .builder\
        .appName("Test")\
        .config('spark.sql.warehouse.dir', 'file:///C:/')\
        .getOrCreate()
sc = spark.sparkContext

#load data
train = spark.read.load("./data/train_small.parquet")
test = spark.read.load("./data/test_small.parquet")

#filter data by class, then aggregate total feature counts per class
most_common = []
for i in range(9):
    data = train.filter(train.label==i)
    sum_bycol = data.select('features').rdd.aggregate(np.zeros(65535),lambda acc,row:acc+row[0].toArray(),lambda acc,row:acc+row)
    max = sum_bycol.argsort()[-1000:][::-1] #keep top 1000 in each class
    most_common.append(max)

#remove duplicate entries (i.e., overlap between classes)
most_common = np.array(most_common)
most_common = np.unique(most_common)

#save to file
print most_common
print len(most_common)
np.save('./data/most_common_bigrams_tfidf', most_common)